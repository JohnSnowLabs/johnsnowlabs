import os
from os.path import expanduser

from johnsnowlabs.utils.env_utils import (
    env_required_license,
    is_running_in_databricks,
    is_running_in_emr,
    set_py4j_logger_to_error_on_databricks,
)

# These versions are used for auto-installs and version  checks


raw_version_jsl_lib = "5.1.7"


raw_version_nlp = "5.1.2"

raw_version_nlu = "5.0.4rc2"



raw_version_pyspark = "3.1.2"
raw_version_nlp_display = "4.1"

raw_version_medical = "5.1.2"
raw_version_secret_medical = "5.1.2"

raw_version_secret_ocr = "5.0.2"
raw_version_ocr = "5.0.2"

raw_version_pydantic = "1.10.11"

pypi_page = "https://pypi.org/project/johnsnowlabs"
json_indent = 4
enforce_secret_on_version = False
enforce_versions = True

# Environment
on_databricks = is_running_in_databricks()
on_emr = is_running_in_emr()
license_required = env_required_license()

# Set root path, from which all other paths will be relative

if on_databricks:
    root_dir = f"/dbfs/johnsnowlabs"
    set_py4j_logger_to_error_on_databricks()
elif on_emr:
    root_dir = f"/lib/.johnsnowlabs/johnsnowlabs"
else:
    home = expanduser("~")
    if home[-1] == "/":
        # on Docker and other setups, ~ may resolve to `/`, so we need special handling here
        root_dir = f'{expanduser("~")}.johnsnowlabs'
    else:
        root_dir = f'{expanduser("~")}/.johnsnowlabs'

## Directories
license_dir = os.path.join(root_dir, "licenses")
java_dir = os.path.join(root_dir, "java_installs")
py_dir = os.path.join(root_dir, "py_installs")

# Info Files
root_info_file = os.path.join(root_dir, "info.json")
java_info_file = os.path.join(java_dir, "info.json")
py_info_file = os.path.join(py_dir, "info.json")
creds_info_file = os.path.join(license_dir, "info.json")

# databricks paths
dbfs_home_dir = "dbfs:/johnsnowlabs"
dbfs_java_dir = f"{dbfs_home_dir}/java_installs"
dbfs_py_dir = f"{dbfs_home_dir}/py_installs"
db_py_jobs_dir = f"{dbfs_home_dir}/py_jobs"
db_py_notebook_dir = f"{dbfs_home_dir}/py_notebook_jobs"
db_jar_jobs_dir = f"{dbfs_home_dir}/jar_jobs"

db_cluster_name = "John-Snow-Labs-Databricks-Auto-ClusterðŸš€"
db_driver_node_type = "i3.xlarge"
db_node_type_id = "i3.xlarge"
db_spark_version = "10.5.x-scala2.12"

# db_spark_version = "13.2.x-scala2.12"

db_job_name = "John-Snow-Labs-Job {job} ðŸš€"
db_run_name = "John-Snow-Labs-Run ðŸš€"
db_driver_node_type = "i3.xlarge"

# emr settings
emr_cluster_name = "John-Snow-Labs-EMR-Auto-ClusterðŸš€"
emr_release_label = "emr-6.5.0"
emr_instance_type = "m4.4xlarge"
emr_instance_count = 3
emr_applications = ["Hadoop", "Spark", "Livy", "JupyterEnterpriseGateway"]
emr_default_instance_profile = "EMR_EC2_DefaultRole"
emr_default_service_role = "EMR_DefaultRole"
emr_volume_size = 100

# Local Spark mode
spark_session_name = "John-Snow-Labs-Spark-Session ðŸš€"
