---
layout: docs
comment: no
header: true
seotitle: Generative AI Lab | John Snow Labs
title: Import Documents
permalink: /docs/en/alab/import
key: docs-training
modify_date: "2020-11-18"
use_language_switcher: "Python-Scala"
show_nav: true
sidebar:
  nav: annotation-lab
---

Once a new project is created and its configuration is saved, the user is redirected to the **Import page**. Here the user has multiple options for importing tasks.

![Generative AI Lab Import Documents](/assets/images/annotation_lab/4.2.0/import.png "lit_shadow")

Users can import the accepted file formats in multiple ways. They can drag and drop the file(s) to the upload box, select the file from the file explorer, provide the URL of the file in JSON format, or import it directly from the S3 bucket. To import from Amazon S3 bucket the user needs to provide the necessary connection details (_credentials_, _access keys_, and _S3 bucket path_). All documents present in the specified path, are then imported as tasks in the current project.

![Generative AI Lab Import Documents](https://user-images.githubusercontent.com/46840490/203529045-14df3352-e8c5-4a7d-9f3a-e152c51e6d43.gif "lit_shadow")

## Plain text file

When you upload a plain text file, only one task will be created which will contain the entire data in the input file.

This is an update from earlier versions of Generative AI Lab when the input text file was split by the new line character and one task was created for each line.
{:.warning}

## Json file

For bulk importing a list of documents you can use the json import option. The expected format is illustrated in the image below. It consists of a list of dictionaries, each with 2 keys-values pairs (“text” and “title”).

```bash
[{"text": "Task text content.", "title":"Task title"}]
```

## CSV, TSV file

When CSV / TSV formatted text file is used, column names are interpreted as task data keys:

```bash
Task text content, Task title
this is a first task, Colon Cancer.txt
this is a second task, Breast radiation therapy.txt
```

## Import annotated tasks

When importing tasks that already contain annotations (e.g. exported from another project, with predictions generated by pre-trained models) the user has the option to overwrite completions/predictions or to skip the tasks that are already imported into the project.

![Import annotated tasks](/assets/images/annotation_lab/4.2.0/overwrite.png "lit_shadow")

> **NOTE:** When importing tasks from different projects with the purpose of combining them in one project, users should take care of the overlaps existing between tasks IDs. Generative AI Lab will simply overwrite tasks with the same ID.

## Dynamic Task Pagination

The support for pagination offered by earlier versions of the Generative AI Lab involved the use of the `<pagebreak>` tag. A document pre-processing step was necessary for adding/changing the page breaks and those involved extra effort from the part of the users.

Generative AI Lab 2.8.0 introduces a paradigm change for pagination. Going forward, pagination is dynamic and can be configured according to the user’s needs and preferences from the Labeling page. Annotators or reviewers can now choose the number of words to include on a single page from a predefined list of values or can add the desired counts.

![Dynamic Task Pagination](/assets/images/annotation_lab/4.2.0/pagination.gif "lit_shadow w_80")

A new settings option has been added to prevent splitting a sentence into two different pages.

![Dynamic Task Pagination](/assets/images/annotation_lab/2.8.0/158552636-1b9f8814-5e05-4904-8ab4-401ea476d32e.png "lit_shadow w_60")

## Import from Cloud Storage
Generative AI Lab 4.3.0 offers support for importing tasks/documents stored on cloud. In the `Import Page`, a new section was added which allows users to define S3 connection details (credentials, access keys, and S3 bucket path). All documents present in the specified path, are imported as tasks in the current Generative AI Lab project. With Version 5.9 of Generative AI Lab allows you to effortlessly import projects using S3 and Azure Blob.


Generative AI Lab 5.8 introduces a pivotal enhancement that expands task management capabilities by seamlessly integrating with Azure Blob storage, complementing the existing support for AWS S3. This integration empowers users to streamline task import and export processes, fostering greater efficiency and flexibility in their data handling workflows within the Generative AI Lab platform.

### Effortless Task Import from Azure Blob Storage:

Importing tasks from Azure storage containers is now as straightforward and intuitive as importing from AWS S3. Follow these simple steps to effortlessly integrate your Azure data into Generative AI Lab projects:
- **Prepare the Azure Source:** Ensure the Azure storage container from which you intend to import tasks is readily accessible and the target files are available. Generative AI Lab can currently accommodate various document types such as text, PDF, images, videos, and sound files.
- **In your Generative AI Lab project:** Navigate to the Task Import page of the project where you wish to import tasks.
- **Select Azure Blob Storage:** Choose the "Azure BLOB" import option by clicking on the corresponding radio button on the Import page.
- **Enter Azure Credentials:** Provide the Azure connection details: Azure Container Name, Azure Account Name, and Azure Account Secret Key.
- **Initiate Import Process:**  Click the "Import" button to seamlessly transfer compatible documents from the specified Azure container into the current Generative AI Lab project.

![Effortless Task Import from Azure Blob Storage](/assets/images/annotation_lab/5.8.0/2.gif)

### Import Project from S3 and Blob

Generative AI Lab 4.3.0 offers support for importing tasks/documents stored on cloud. In the `Import Page`, a new section was added which allows users to define S3 connection details (credentials, access keys, and S3 bucket path). All documents present in the specified path, are imported as tasks in the current Generative AI Lab project. With Version 5.9 of Generative AI Lab allows you to effortlessly import projects using S3 and Azure Blob.

**Steps to import a project from S3:**
- Navigate to "Import Project"
- Choose "AWS S3"
- Input the path to the S3 file as s3://bucket/folder/file.zip
- Provide S3 Access Key, S3 Secret Key, and Session Token (Required for MFA Accounts)
- Click "Import"
  ![S3_import](/assets/images/annotation_lab/5.9.0/15.gif)


**Steps to import a project from Azure Bbob:**
- Go to "Import Project"
- Select "Azure Blob"
- Enter the path to the Azure Blob file as Container/file.zip
- Input Azure Account Name and Azure Account Secret Key
- Click "Import"
 
  ![Import_azure](/assets/images/annotation_lab/5.9.0/16.gif)

## Cloud Storage Credential Management
**What's New:** Cloud storage credentials (AWS S3, Azure Blob Storage) can now be saved at the project level, eliminating repetitive credential entry while maintaining security isolation.

**Technical Implementation:**
- Credentials stored per-project, not globally
- Automatic credential reuse for subsequent import/export operations within the same project
- Dedicated UI controls for managing credentials during Import and Export
- Credentials excluded from project ZIP exports for security compliance
- Support for credential updates through explicit save actions

![730image](/assets/images/annotation_lab/7.3.0/6.gif)

**User Benefits:**
- **Data Scientists:** Eliminates manual credential re-entry for frequent data operations, reducing setup time by 60-80% for iterative workflows
- **Multi-Team Organizations:** Each project team can manage its cloud access without sharing credentials
- **DevOps Teams:** Reduces credential management overhead while maintaining security boundaries

![730image](/assets/images/annotation_lab/7.3.0/7.gif)

**Note:-** Credentials, once saved, will remain associated with the project and will be auto-filled when revisiting the Import or Export pages—even if a different path or new credentials are used temporarily. To update them, users must explicitly choose to save new credentials.*

**Note:-** If credentials are saved for a different cloud provider within the same project (e.g., switching from AWS to Azure), the previously stored credentials will be overwritten with the new set.

**Example Use Case:** A medical coding organization can save their S3 credentials once to import daily batches of clinical documents for human-in-the-loop validation, while simultaneously maintaining separate Azure credentials for exporting coded results to their analytics platform—eliminating daily credential re-entry across both cloud providers.

**Technical Implementation:**
- Credentials stored per-project, not globally
- Automatic credential reuse for subsequent import/export operations within the same project
- Dedicated UI controls for managing credentials during Import and Export
- Credentials excluded from project ZIP exports for security compliance
- Support for credential updates through explicit save actions

![730image](/assets/images/annotation_lab/7.3.0/6.gif)

**User Benefits:**
- **Data Scientists:** Eliminates manual credential re-entry for frequent data operations, reducing setup time by 60-80% for iterative workflows
- **Multi-Team Organizations:** Each project team can manage its cloud access without sharing credentials
- **DevOps Teams:** Reduces credential management overhead while maintaining security boundaries

![730image](/assets/images/annotation_lab/7.3.0/7.gif)

**Note:-** Credentials, once saved, will remain associated with the project and will be auto-filled when revisiting the Import or Export pages—even if a different path or new credentials are used temporarily. To update them, users must explicitly choose to save new credentials.*

**Note:-** If credentials are saved for a different cloud provider within the same project (e.g., switching from AWS to Azure), the previously stored credentials will be overwritten with the new set.

**Example Use Case:** A medical coding organization can save their S3 credentials once to import daily batches of clinical documents for human-in-the-loop validation, while simultaneously maintaining separate Azure credentials for exporting coded results to their analytics platform—eliminating daily credential re-entry across both cloud providers.## Cloud Storage Credential Management
**What's New:** Cloud storage credentials (AWS S3, Azure Blob Storage) can now be saved at the project level, eliminating repetitive credential entry while maintaining security isolation.

**Technical Implementation:**
- Credentials stored per-project, not globally
- Automatic credential reuse for subsequent import/export operations within the same project
- Dedicated UI controls for managing credentials during Import and Export
- Credentials excluded from project ZIP exports for security compliance
- Support for credential updates through explicit save actions

![730image](/assets/images/annotation_lab/7.3.0/6.gif)

**User Benefits:**
- **Data Scientists:** Eliminates manual credential re-entry for frequent data operations, reducing setup time by 60-80% for iterative workflows
- **Multi-Team Organizations:** Each project team can manage its cloud access without sharing credentials
- **DevOps Teams:** Reduces credential management overhead while maintaining security boundaries

![730image](/assets/images/annotation_lab/7.3.0/7.gif)

**Note:-** Credentials, once saved, will remain associated with the project and will be auto-filled when revisiting the Import or Export pages—even if a different path or new credentials are used temporarily. To update them, users must explicitly choose to save new credentials.*

**Note:-** If credentials are saved for a different cloud provider within the same project (e.g., switching from AWS to Azure), the previously stored credentials will be overwritten with the new set.

**Example Use Case:** A medical coding organization can save their S3 credentials once to import daily batches of clinical documents for human-in-the-loop validation, while simultaneously maintaining separate Azure credentials for exporting coded results to their analytics platform—eliminating daily credential re-entry across both cloud providers.

### Import for Large Datasets
The background processing architecture now handles large-scale imports without UI disruption through intelligent format detection and dynamic resource allocation. When users upload tasks as a ZIP file or through a cloud source, Generative AI Lab automatically detects the format and uses the import server to handle the data in the background — ensuring smooth and efficient processing, even for large volumes. 

For smaller, individual files — whether selected manually or added via drag-and-drop — imports are handled directly without background processing, allowing for quick and immediate task creation.

**Note:** Background import is applied only for ZIP and cloud-based imports.

**Automatic Processing Mode Selection:**
- ZIP files and cloud-based imports: Automatically routed to background processing via dedicated import server
-  Individual files (manual selection or drag-and-drop): Processed directly for immediate task creation
- The system dynamically determines optimal processing path based on import source and volume

![720image](/assets/images/annotation_lab/7.2.0/8.png)

**Technical Architecture:**
- Dedicated import cluster with auto-provisioning: 2 CPUs, 5GB memory (non-configurable)
- Cluster spins up automatically during ZIP and cloud imports
- Automatic deallocation upon completion to optimize resource utilization
- Sequential file processing methodology reduces system load and improves reliability
- Import status is tracked and visible on the Import page, allowing users to easily monitor
progress and confirm successful uploads.

![720image](/assets/images/annotation_lab/7.2.0/9.png)

**Performance Improvements:**
- Large dataset imports (5000+ files): Previously 20+ minutes, now less than 10 minutes
- Elimination of UI freezing during bulk operations
- Improved system stability under high-volume import loads

***Note: Import server created during task import is counted as an active server.***

## Disabled Local Imports
What's New: Administrators can now disable local file imports system-wide, complementing existing local export restrictions to create complete data flow control.

**Technical Implementation:**
- New "Disable Local Import" setting in System Settings → General tab
- Project-level exceptions are available through the dedicated Exceptions widget
- When this option is enabled, only cloud storage imports (Amazon S3, Azure Blob Storage) are permitted.
- Setting applies globally across all projects unless explicitly exempted

![730image](/assets/images/annotation_lab/7.3.0/1.png)

**User Benefits:**
- **Healthcare Organizations:** Ensures all patient data flows through auditable, encrypted cloud channels rather than local file systems.
- **Enterprise Teams:** Eliminates the risk of sensitive data being imported from uncontrolled local sources.
- **Compliance Officers:** Provides granular control over data ingress while maintaining operational flexibility for approved projects.

![730image](/assets/images/annotation_lab/7.3.0/2.png)

**Example Use Case:** A healthcare system can disable local imports for all PHI processing projects while maintaining exceptions for internal development projects that use synthetic data.