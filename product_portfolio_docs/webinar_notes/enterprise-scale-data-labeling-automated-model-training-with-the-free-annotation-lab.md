# Enterprise-Scale Data Labeling & Automated Model Training with the Free Annotation Lab
Enterprise-Scale Data Labeling & Automated Model Training with the Free Annotation Lab

<https://www.johnsnowlabs.com/watch-webinar-enterprise-scale-data-labeling-automated-model-training-with-the-free-annotation-lab/>

<https://www.youtube.com/watch?v=Nvwh6236wA0>

<img src="/media/image.jpg" title="Video titled: Enterprise-Scale Data Labeling &amp; Automated Model Training with the Free Annotation Lab" style="width:6.3125in;height:3.65625in" />

The speech provided in the excerpts from the John Snow Labs webinar focuses on the features, workflow, and technical capabilities of the **Annotation Lab**, a **free tool** for **enterprise-scale data labeling** and **automated model training**.

Below is a detailed summary of the key points covered:

### **Annotation Lab Introduction and Availability**

Annotation Lab is a **free tool/software** designed for **data labeling for enterprise and big companies**. The offering is comprehensive, as the pre-annotation by JSON models, training, active learning, project analysis dashboard, and project sharing with any number of users are **absolutely free of cost**. There is **no limit** on the number of users or projects.

The tool supports labeling for various project types, including **text, image, audio, PDF files, and videos**.

### **Security, Privacy, and Compliance**

Annotation Lab is suitable for **high compliance environments**. A critical feature is its ability to be installed in a **completely air-gapped environment**, eliminating the need to communicate with the internet so that **no data leaves your machine**.

To ensure security, John Snow Labs performs continuous security measures for its releases, including running **steady code analysis, pen testing, and scanning all images**. Frequent releases occur every 15 to 20 days to ship security fixes and improvements.

In terms of data access, the system is designed so that a person **cannot see a project unless that project is assigned to them**, regardless of their role (even if they are an owner, manager, or admin user admin of another project). This is crucial for keeping high-compliance data safe.

### **Key Terminology**

The speaker defines essential terms used within the Annotation Lab platform:

- **Task:** This is the **work unit that can be assigned to someone**. A task does not always equate to a single imported document; it can be a merger or combination of multiple documents.

- **Completion:** This refers simply to **a work done within a task**. A single task can contain multiple completions from one or multiple users.

- **Prediction (Pre-annotation):** This is the pre-annotation work generated by the **model server**.

### **Workflow Overview and Management**

The general workflow involves several steps:

1.  **Project Creation and Setup:** Creating the project, optionally downloading models for pre-annotation, configuring the project layout and type, and inviting team members.

2.  **Importing Documents:** Documents are imported using various supported formats (JSON, CSV, TXT, zip).

3.  **Pre-annotation (Optional):** Tasks can be selected, and the "pre annotate" button can be used to leverage the model server.

4.  **Assignment and Annotation:** Tasks are assigned to team members (annotators).

5.  **Review and Analysis:** Work is reviewed, and the analytics dashboard is used to analyze project work.

6.  **Export and Training:** Data can be exported in preferred formats (like CSV and JSON). Alternatively, good annotated documents can be used to **train models within the tool without writing code**.

7.  **Iteration:** If results are unsatisfactory, reviewers can give feedback within the tool, and the process is repeated.

Managing numerous projects (tens, twenties, or fifties) is made easier through **grouping projects**. The platform also supports **SSO** (Single Sign-On), allowing users to access the lab without recreating accounts if they already exist in a server or AD.

### **Project Roles and Configuration**

Annotation Lab defines four primary roles: **Annotator** (does annotation), **Reviewer** (reviews work), **Manager** (manages overall project settings and assignments), and **Owner** (creates the project).

The project setup page allows configuration via four tabs, including the **Project Configuration** and the **Training and Active Learning** tabs.

- **Configuration Details:** The tool offers **more than 40 default templates** for various project types. Users can configure **predefined labels** by listing available models, which can then be deployed for pre-annotation. The project structure can be configured through a detailed **Code View (using XML)** or a simpler **Visual View** with input fields for defining labels and classes.

- **APIs:** Everything achievable through the user interface (UI) can also be done via **APIs**. The API documentation (Swagger docs) is fully documented and accessible via a menu on the bottom-left corner of the installation.

### **Annotation Experience and Tracking**

The annotation page is highly customizable, allowing for different layouts per project or per user. Users can adjust the position of completion and result panels (bottom, left, or right).

Key features include:

- **Relation Annotation:** Relations between tokens can be easily created using shortcuts (e.g., pressing 'R').

- **Comparison:** It is possible to **compare between two or more completions** (e.g., comparing a user's work against a model's prediction/pre-annotation).

- **Large Documents:** For large documents, **pagination** is available and recommended.

- **Audit Trail:** The lab maintains a **complete trail** of who has performed specific actions. Submitted tasks cannot be deleted. It tracks how a completion was created (e.g., copied from another completion or copied from a prediction).

### **Analytics Dashboard**

The dashboard provides charts for in-depth analysis of the project. For example, a chart can show the **agreement percentage** between two annotators (e.g., Jane and John agreed on 85% of tokens). Comparisons are also available per label or per chunk. Users can download CSV files containing the data shown in the charts.

### **Active Learning (Automated Model Training)**

Active Learning is the central theme of the presentation and is a **no-code environment** feature. It implements a **human-in-the-loop workflow** without requiring any coding.

**Active Learning Process:**

1.  An annotator completes and submits a document.

2.  A reviewer reviews and marks the document as 'done'.

3.  The document is added to the training chunk.

4.  Once a pre-defined number of completed documents (\$N\$, which can be configured-e.g., **50 completions**) is reached, an **automatic training job is triggered**.

5.  A new, improved model is created, deployed, and then used again for pre-annotating subsequent documents.

Users configure the training parameters (like epochs or learning rate) and can filter which tasks or completions are used for training (e.g., only "submitted" or "reviewed" tasks). It is also possible to upload a **custom training script**. Users can also select and assign specific documents to be used as a **test dataset** or a **train dataset**.

### **Demo Summary**

The speaker provided two main demonstrations:

1.  **NER Pre-annotation:** A project was set up using a pre-existing movie model (ner mit movie). The speaker customized the project by adding a new label, **"revenue,"** which was not part of the base model. The model was deployed, and when the "pre-annotate" button was hit, the model server provided predictions on the imported document. The user could then copy the prediction and add the custom "revenue" annotation.

2.  **Active Learning Setup:** A project was created with three labels (medicine, medical condition, pathogen). The **Active Learning feature was enabled** and configured to trigger training after every **50 completions**. After importing 50 documents (49 previously annotated, 1 new), the final document was annotated and submitted, which caused the **training job to be automatically triggered**. The speaker showed that the model was successfully trained and deployed.